# 3D face
# ----------------Moving Mnist-------------
观察 G和D中norm的作用

>>>3dmmnist_wmw+_cd20_cc20_noNorm
#取消了GD的fc的norm和conv最后一层的norm, 结果：模式崩溃，单一图片

>>> 3dmmnist_wmw+_cd20_cc20_noNormD
#取消了D的fc的norm和conv最后一层的norm,  结果：模式对应了，但是结果比D加了batch_norm的粗糙, 不过后面恢复了

>>> 3dmmnist_wmw+_cd20_cc20_noNormD_L1
# conLoss 的 mse 去掉平方,  类似WGAN, 不加gp全黑

>>> 3dmmnist_wmw+_cd20_cc20_noNormD_L1_gp
# 原因是wgan的loss一直在增大，只有Clipping才行，但也达不到纳什均衡

>>>3dmmnist_wmw+_cd20_cc20_lamb20_balence
# 对称的norm和activition , conLoss乘以20 结果崩了


#标准版 D的fc最后一层，即输出层，即没有norm，也没有activation，原因其是一个判别器，是一个概率输出模型

